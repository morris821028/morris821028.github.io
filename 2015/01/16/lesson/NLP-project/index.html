<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>自然語言處理 Project | Morris&#39; Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Polarity Analysis for Sentiment ClassificationStop Word ListThis stopword list is probably the most widely used stopword list. It covers a wide number of stopwords without getting too aggressive and i">
<meta name="keywords" content="自然語言">
<meta property="og:type" content="article">
<meta property="og:title" content="自然語言處理 Project">
<meta property="og:url" content="http://morris821028.github.io/2015/01/16/lesson/NLP-project/index.html">
<meta property="og:site_name" content="Morris&#39; Blog">
<meta property="og:description" content="Polarity Analysis for Sentiment ClassificationStop Word ListThis stopword list is probably the most widely used stopword list. It covers a wide number of stopwords without getting too aggressive and i">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2021-05-30T02:04:42.506Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="自然語言處理 Project">
<meta name="twitter:description" content="Polarity Analysis for Sentiment ClassificationStop Word ListThis stopword list is probably the most widely used stopword list. It covers a wide number of stopwords without getting too aggressive and i">
<link rel="publisher" href="108158678174364350000">
  
    <link rel="alternative" href="/atom.xml" title="Morris&#39; Blog" type="application/atom+xml">
  
  
    <meta name="google-site-verification" content="5mRgj8NanEMpGZuNfHNJNmH90RgNlrnJXsFlTaKD6Gs" />
  
  
    <link rel="shortcut icon" href="/img/f.ico">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <script src="/js/jquery-2.1.0.min.js"></script>
  <link rel="stylesheet" href="/css/style.css">
  
  <link rel="stylesheet" href="http://cdn.oesmith.co.uk/morris-0.5.1.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/raphael/2.1.0/raphael-min.js"></script>
<script src="http://cdn.oesmith.co.uk/morris-0.5.1.min.js"></script>
  <link rel="import" href="/bower_components/app-layout/app-layout.html"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"><div id="banner-right"></div></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Morris&#39; Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          
            <a class="main-nav-link" href="/"><i class=icon-home title='Home'></i></a>
          
        
          
            <a class="main-nav-link" href="/about"><i class=icon-user title='About'></i></a>
          
        
          
            <a class="main-nav-link" href="/archives"><i class=icon-archive title='Archives'></i></a>
          
        
          
            <a class="main-nav-link" href="/tags"><i class=icon-tags title='Tags'></i></a>
          
        
          
            <a class="main-nav-link" href="/picture"><i class=icon-camera title='Pictures'></i></a>
          
        
          
            <a class="main-nav-link" href="/works"><i class=icon-trophy title='Works'></i></a>
          
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://morris821028.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        
          
          <section id="main" style="width: 95%"><article id="post-lesson/NLP-project" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/01/16/lesson/NLP-project/" class="article-date">
  <time datetime="2015-01-16T00:15:31.000Z" itemprop="datePublished">2015-01-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/學校課程/">學校課程</a>/<a class="article-category-link" href="/categories/學校課程/自然語言/">自然語言</a>
  </div>

  </div>
  <div class="article-inner ">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      自然語言處理 Project
    </h1>
  

      </header>
    
    <footer class="article-footer">
      <a data-url="http://morris821028.github.io/2015/01/16/lesson/NLP-project/" data-id="ckpaivcae00c5n8vnevdv1ode" class="article-share-link">Share</a>
      
        <a href="http://morris821028.github.io/2015/01/16/lesson/NLP-project/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/自然語言/">自然語言</a></li></ul>

    </footer>
    <div class="article-entry " itemprop="articleBody">
      
          
              <div id="toc" class="toc-article">
              <h2 class="toc-title"><span>contents</span></h2>
              
                  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Polarity-Analysis-for-Sentiment-Classification"><span class="toc-number">1.</span> <span class="toc-text">Polarity Analysis for Sentiment Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Stop-Word-List"><span class="toc-number">1.1.</span> <span class="toc-text">Stop Word List</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Parsing-Sentence"><span class="toc-number">1.2.</span> <span class="toc-text">Parsing Sentence</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Filter-N-grams-Feature"><span class="toc-number">1.3.</span> <span class="toc-text">Filter N-grams Feature</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#How-To-Decide-K-top"><span class="toc-number">1.4.</span> <span class="toc-text">How To Decide K-top</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Improve"><span class="toc-number">1.4.1.</span> <span class="toc-text">Improve</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#LM-Classifier"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">LM Classifier</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Combine-Classifier"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">Combine Classifier</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#N-grams-Score"><span class="toc-number">1.4.1.3.</span> <span class="toc-text">N-grams Score</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Vector"><span class="toc-number">1.4.1.4.</span> <span class="toc-text">Vector</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Process"><span class="toc-number">1.4.1.5.</span> <span class="toc-text">Process</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#extra-data-support"><span class="toc-number">1.5.</span> <span class="toc-text">extra data support</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#程式撰寫"><span class="toc-number">2.</span> <span class="toc-text">程式撰寫</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#N-grams-Storing"><span class="toc-number">2.1.</span> <span class="toc-text">N-grams Storing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#How-to-improve-experiment"><span class="toc-number">2.2.</span> <span class="toc-text">How to improve experiment</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pseudocode"><span class="toc-number">2.3.</span> <span class="toc-text">Pseudocode</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#preprocess"><span class="toc-number">2.3.1.</span> <span class="toc-text">preprocess</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#online-test"><span class="toc-number">2.3.2.</span> <span class="toc-text">online test</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fail-online-test-idea"><span class="toc-number">2.3.3.</span> <span class="toc-text">fail online test idea</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Support-N-grams-Sieve"><span class="toc-number">2.3.4.</span> <span class="toc-text">Support N-grams Sieve</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#To-Do"><span class="toc-number">3.</span> <span class="toc-text">To Do</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Github"><span class="toc-number">4.</span> <span class="toc-text">Github</span></a></li></ol>
              
              </div>
          
        
          <h1 id="Polarity-Analysis-for-Sentiment-Classification"><a href="#Polarity-Analysis-for-Sentiment-Classification" class="headerlink" title="Polarity Analysis for Sentiment Classification"></a>Polarity Analysis for Sentiment Classification</h1><h2 id="Stop-Word-List"><a href="#Stop-Word-List" class="headerlink" title="Stop Word List"></a>Stop Word List</h2><p>This stopword list is probably the most widely used stopword list. It covers a wide number of stopwords without getting too aggressive and including too many words which a user might search upon. This wordlist contains only 11 words.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">the</span><br><span class="line">i</span><br><span class="line">it</span><br><span class="line">he</span><br><span class="line">she</span><br><span class="line">they</span><br><span class="line">we</span><br><span class="line">you</span><br><span class="line">-</span><br><span class="line">a</span><br><span class="line">an</span><br></pre></td></tr></table></figure>

<p>比起其他網路上的 stop word list 少了很多介系詞，原因是這樣的，例如 <code>who, whom, where, ...</code> 這些詞作為描述對象的主體，這些主體通常不可省略。例如在描述『演員很不錯，但是電影劇本很糟糕。』這段話時，根據實驗的幾種模型，如果忽略主體的存在，很容易造成描述的需求面向錯誤。必然地，無法將 <code>movie, film</code> 過濾掉，主體是相當重要的，即使他在單一詞的極性上不夠明確。</p>
<h2 id="Parsing-Sentence"><a href="#Parsing-Sentence" class="headerlink" title="Parsing Sentence"></a>Parsing Sentence</h2><p>首先，必須把所有縮寫單位展開，當然有可能把錯誤的 <code>Bob&#39;s</code> 展開成錯誤的意思，實驗上不構成多大的影響，但以下的操作會更明確地把潛在的 feature 更清楚地劃分，而不會挑到已經重複的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">`can&#x27;t` = `can not`</span><br><span class="line">`n&#x27;t` = ` not`</span><br><span class="line">`&#x27;re` = ` are`</span><br><span class="line">`&#x27;m` = ` am`</span><br><span class="line">`&#x27;s` = ` is`</span><br><span class="line">`&#x27;ve` = ` have`</span><br><span class="line">`&#x27;ll` = ` will`</span><br><span class="line">... maybe more</span><br></pre></td></tr></table></figure>

<p>再來，必須針對符號轉英詞，有可能是在描述電影名稱會用到 <code>&amp;</code>，一律將其轉換成 <code>and</code> 會更有效。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`&amp;` = ` and`</span><br></pre></td></tr></table></figure>

<p>最後，語法上的變換統一。這可能會遭遇到描述『過去的版本很好，現在這個版本很糟。』也許規則應該對時間做點細部劃分。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">`am, are, is, was, were` = `be`</span><br><span class="line">`no` = `not`</span><br><span class="line">`film` = `movie`</span><br></pre></td></tr></table></figure>

<p>更多的口語描述，例如 <code>uuuuuuuugggggggggglllllllllllyyyyyyy = ugly</code> 的可能，特別針對 long duplicate word 特別處理，查看濃縮之後，是否會在字典中。狀聲詞 <code>oh, wow, ah, ...</code> 可能是一個極性指標，在此就不濾掉這幾個單詞。</p>
<p>當使用 stop word 過濾某一個句子時，剩餘的詞應該串在一起成為新的句子，而不以 stop word 分開成新的句子。</p>
<h2 id="Filter-N-grams-Feature"><a href="#Filter-N-grams-Feature" class="headerlink" title="Filter N-grams Feature"></a>Filter N-grams Feature</h2><p>在挑選 n-grams 時，根據給定的公式，從 800 正向評論、800 反向評論中，大約會得到 50M ~ 100M 不同的 n-grams。當我們篩選 n = 3 時，<code>bad</code> 將可能被儲存為 <code>(bad, null, null)</code>。挑選時，必須保障 high order n-grams 佔有一定的數量，大約落在 <code>n-grams : (n+1)-grams = 7 : 3</code>。</p>
<p>評分時，額外增加 high order 的評分權重，以下是程式中使用的分配。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">N-grams Bonus = &#123;1, 1.02, 1.03, 1.04, 1.05, 1.06, 1.07&#125;;</span><br></pre></td></tr></table></figure>

<p>這麼做確保實驗上有保留之前的優良，同時也增加新的元素進去。</p>
<p>當挑選 K-top feature 時，必須將正反兩方的 feature N-grams 分別佔有約 50%，並且去掉同時挑到的情況。</p>
<p>更多的策略，例如 <code>(bad, too, null)</code> 可以視為 <code>(too, bad, null)</code>，使用 dag 的方式紀錄 N-grams。</p>
<h2 id="How-To-Decide-K-top"><a href="#How-To-Decide-K-top" class="headerlink" title="How To Decide K-top"></a>How To Decide K-top</h2><p>明顯地觀察到，K 越大時，Language Model 越穩定，平均效能穩定，但最好效能可能會下降，對於 Winnow algorithm 或者是 Passive-Aggressive algorithm 來說，feature vector 會非常大，造成在找到合適的參數會消耗更多的時間，並且難在有限的迭代次數中找到好的切平面。</p>
<p>當 K 很大時，部分的 n-grams 甚至只出現在某些的 training set 中，這造成訓練 Winnow, Passive-Aggressive 的結果並不好，過於分配無用的權重給只出現在 training set 的 feature。在實驗中，挑選 K = 30000 與 K = 50000 的差異並不大，挑選的比例約為 5% 以內。</p>
<h3 id="Improve"><a href="#Improve" class="headerlink" title="Improve"></a>Improve</h3><h4 id="LM-Classifier"><a href="#LM-Classifier" class="headerlink" title="LM Classifier"></a>LM Classifier</h4><p>關於論文中提到的 LM filter，試圖去替除掉一些主觀句子，根據每一個句子進行閥值的評測，實驗中測試到非常低的值作為閥值才具有較好的可能性，由於太高、太低都會使得準確度下降，估計這個閥值的調整不具有普遍性。</p>
<p>而其他的 Classifier 並沒有做這類的主客觀篩選，也許可以藉由主客觀句子的判斷做一套分類器，說不定可以改善另外兩個分類器的語意判斷能力。</p>
<h4 id="Combine-Classifier"><a href="#Combine-Classifier" class="headerlink" title="Combine Classifier"></a>Combine Classifier</h4><p>發現到 Winnow, Passive-Aggressive (PA) 都是以閥值作為判斷標準，因此當得到靠近閥值的數據下，判斷能力是相當脆弱。雖然 PA 在平均表現最好，大約落在 85% 附近，遠比 LM 和 Winnow 多了 5% ~ 10% 的準確度，如何加強？ </p>
<p>將四個 Classifier 串在一起，並且用八個 attribute 作為一個 feature vector，接著訓練這一個感知機。這八個 attribute 分別是每一個 Classifier 的判斷強度。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(LM_POS, LM_NEG, WINNOW_POS, WINNOW_NEG, PA_POS, PA_NEG, SIMPLE_POS, SIMPLE_NEG)</span><br></pre></td></tr></table></figure>

<p>其中由於 Language Model (LM) 的機率不好量化，因此單純採用 0/1 的方式表示。嘗試過取 log 發現仍然並不好。</p>
<ul>
<li>LM_POS : if LM.classify(x) = POS, then LM_POS = 1. Otherwise LM_POS = 0</li>
<li>LM_NEG : if LM.classify(x) = NEG, then LM_NEG = 1. Otherwise LM_POS = 0</li>
<li>WINNOW_POS : if Winnow.classify(x) = POS, the WINNOW_POS = Winnow.strongClassify(x).</li>
<li>WINNOW_NEG : if Winnow.classify(x) = NEG, the WINNOW_NEG = Winnow.strongClassify(x).</li>
</ul>
<p>strongClassify(x) 從 training data 中得到 h(x) 的出現的最大值，然後根據將判斷的函數大小得到，<code>strongClassify(x) = h(x) / TRAINING_MAX_H_VALUE</code>，之所以不直接使用 <code>strongClassify(x) = h(x)</code> 是因為很容易造成 overflow 或者是過度的調整判斷。在實驗結果後，將後者公式調整為前者所使用的。</p>
<p>當然可以訓練多台，並且串在一起，但是這種串法必須盡可能有所歧異性，並不是串越多越好，可以藉較少次的迭代次數、洗牌後的訓練序列來達到歧異性。PA 具有良好的適應性，在訓練集與測資集大小、差異不大時，效能仍然可以保持著線性關係，相當具有魯棒性。 </p>
<p>在實驗觀察中可以明白感知器在越少 feature 下，可以在越少迭代次數中訓練完成，相對地適應能力就會隨差異嚴重波動，實驗中使用的幾個感知機模型，都能在 vector 得到後的幾秒內完成訓練，不用勞費 SVM 的數個小時。Language Model 則會因為 feature 越多，展現更加穩定的效能，即便如此，LM 在負面評論的辨識率仍然不高，這一點從論文中也可以看得出具有相同的現象。</p>
<p>藉此把 LM 對於負面評論辨識率很差的特性，才將其判斷與其他的感知機串在一起使用。這一類的串許多的分類器的算法，可以參照 Adaboost (Adaptive Boosting) 的想法。</p>
<h4 id="N-grams-Score"><a href="#N-grams-Score" class="headerlink" title="N-grams Score"></a>N-grams Score</h4><p>特別小心公式的計算，雖然有很多乘除法，可以使用 <code>Math.log</code> 降下來，防止 overflow 的可能，但同時也會造成嚴重的浮點數誤差。所以使用恰當的 <code>double</code> 運算即可，即使遇到 <code>NaN</code> 也沒有關係。</p>
<p>論文中提及的公式，額外增加變成</p>
<span>$\chi^{2}(t, c) = \frac{N \times (AD - CB)^{2} }{(A+C)\times (B + D) \times (A + B) \times (C + D)} \times Weight[t.getSize()] \times Score(t)$</span><!-- Has MathJax -->

<p>其中，<code>Weight[t.getSize()]</code> 正如上述的 <code>N-grams Bonus</code>，當最大上為 <code>n = 5</code> 時，部分 unigram、bigram 仍然有用。而 <code>Score(t)</code> 則是取額外資料中的 <code>AFINN-111.txt</code> 單詞正反面的絕對值權重和，有可能正負兩個詞合併成 bigram 來描述更強烈的負面，因此必須取絕對值。</p>
<h4 id="Vector"><a href="#Vector" class="headerlink" title="Vector"></a>Vector</h4><p>選擇 K-top feature n-grams 後，感知機的 Vector 如何放置權重仍然是個困難，從實驗中，單純拿 <code>n-grams appear times</code> 作為一個 attribute weight 效果並不好，於是嘗試拿 <code>Math.log(n-grams appear times)</code>，但是效果並不好，有可能是浮點數誤差造成的差異並不大，而 <code>Math.log</code> 本身就很小，尤其是 <code>n-grams appear times = 1</code> 的時候會變成 0，額外加上一個基底 base 來補足也拿以取捨。</p>
<p>最後取用</p>
<span>$vector[i] = Score(ngrams(i)) + \sqrt{n-grams(i) \text{ appear times}}$</span><!-- Has MathJax -->

<p>這部分仍然要做實驗，<code>Score(ngrams(i))</code> 大約落在 <code>1 ~ 10</code> 之間。</p>
<h4 id="Process"><a href="#Process" class="headerlink" title="Process"></a>Process</h4><p>經過幾次的 cross validation 後，每一次會挑到不同的 feature n-grams，藉由交叉驗證得到不同的精準度 P，同時也將挑選的 n-grams 增加 P 的權重，在實驗中總共做了 5 次 cross validation，針對同一組 800 資料，進行 <code>1 : 1</code> 的劃分。原本預期挑選 40K 個不同的 n-grams 作為 feature，但是經過 5 次實驗，總共得到 50K 個不同的 n-grams，根據累加的 P 值進行由大排到小，挑選 1/5 的 n-grams 出來，最後挑了少於 10K 個做為 feature。</p>
<p>針對已知的 1600 筆資料進行 <code>3 : 1</code> 的劃分，先對數量較多的資料重訓練，隨後才將數量較少放在一起做第二次訓練。防止過度的訓練，導致感知器針對訓練集的已知資訊分配過多的權重，反而針對未知的元素不足以判斷。</p>
<h2 id="extra-data-support"><a href="#extra-data-support" class="headerlink" title="extra data support"></a>extra data support</h2><ul>
<li>AFINN-111.txt</li>
<li>positive word list (ignore) 毫無幫助</li>
<li>negative word list (ignore) 毫無幫助</li>
<li>negation not (ignore) 目前發現只會更糟糕</li>
</ul>
<h1 id="程式撰寫"><a href="#程式撰寫" class="headerlink" title="程式撰寫"></a>程式撰寫</h1><h2 id="N-grams-Storing"><a href="#N-grams-Storing" class="headerlink" title="N-grams Storing"></a>N-grams Storing</h2><p><code>string</code> 轉 <code>integer</code> 標記。</p>
<h2 id="How-to-improve-experiment"><a href="#How-to-improve-experiment" class="headerlink" title="How to improve experiment"></a>How to improve experiment</h2><p>先用同一份資料訓練、測試，查看是否接近 <code>P = R = 100%</code>，接著放入未知的資料，找到挑選 feature n-grams 之間的差異。</p>
<p>列出幾個可能的差異後，從訓練的感知機中得到每一項的權重，由於是線性分類器，權重的大小即可作為是否具有特色，通常差距會達到 10 ~ 100 倍之間。即使從 N-grams score 得到較高的分數，從感知機中會發現到未必是較大的權重，有可能是某幾篇相關的電影所造成的一面倒。</p>
<h2 id="Pseudocode"><a href="#Pseudocode" class="headerlink" title="Pseudocode"></a>Pseudocode</h2><h3 id="preprocess"><a href="#preprocess" class="headerlink" title="preprocess"></a>preprocess</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">gobal_record(n-grams, score)</span><br><span class="line">for i  = 0 to CROSS_VALIDATION_MAX</span><br><span class="line">    shuffle_order(training_data)</span><br><span class="line">    (ttraining, ttest) = split(training_data, 1 : 1)</span><br><span class="line">    Vectraining = n-grams_sieve(ttraining)</span><br><span class="line">    (LM, Winnow, PA) = training(Vectraining)</span><br><span class="line">    P = test(LM, Winnow, PA, Vectraining)</span><br><span class="line">    gobal_record.add(P, Vectraining)</span><br><span class="line"> </span><br><span class="line">sort gobal_record(n-grams, score) by descending order.</span><br><span class="line">shuffle_order(training_data)</span><br><span class="line"></span><br><span class="line">(ttraining, tretain) = split(training_data, 4 : 1)</span><br><span class="line">N-gramsTable = gobel_record.sublist(K)</span><br><span class="line">Vectraining = parsingInput(ttraining,, N-gramsTable)</span><br><span class="line">Vecretain= parsingInput(tretain,, N-gramsTable)</span><br><span class="line"></span><br><span class="line">(LM, Winnow, PA) = training(Vectraining)</span><br><span class="line">(LM, Winnow, PA) = retraining(Vecretain)</span><br><span class="line"></span><br><span class="line">simpleTest(testdata, LM, Winnow, PA)</span><br><span class="line">onlineTest(testdata, LM, Winnow, PA)</span><br></pre></td></tr></table></figure>

<h3 id="online-test"><a href="#online-test" class="headerlink" title="online test"></a>online test</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">GROUP_SIZE = 50</span><br><span class="line">groups = splitEach(testdata, GROUP_SIZE)</span><br><span class="line">foreach(group : groups)</span><br><span class="line">    appear set = find(group, N-gramsTable)</span><br><span class="line">    (LM, Winnow, PA) = retrainingLimited(Vectraining, appear set)</span><br><span class="line">    foreach(data : group)</span><br><span class="line">        classify(LM, Winnow, PA, data)	</span><br></pre></td></tr></table></figure>

<h3 id="fail-online-test-idea"><a href="#fail-online-test-idea" class="headerlink" title="fail online test idea"></a>fail online test idea</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">GROUP_SIZE = 10</span><br><span class="line">groups = splitEach(testdata, GROUP_SIZE)</span><br><span class="line">foreach(group : groups)</span><br><span class="line">    backtracking 210 possible solution</span><br><span class="line">        training &amp; test</span><br><span class="line">        if (better performance)</span><br><span class="line">            record solution.</span><br><span class="line">Work, but not helpful.	</span><br></pre></td></tr></table></figure>

<h3 id="Support-N-grams-Sieve"><a href="#Support-N-grams-Sieve" class="headerlink" title="Support N-grams Sieve"></a>Support N-grams Sieve</h3><ul>
<li>AFINN-111.txt<br>The file AFINN-111.txt contains a list of sentiment scores</li>
<li>Stop word list<br>Small set, |S| &lt; 20</li>
<li>Synonymous “Not” list<br>unused</li>
<li>Abbreviation list<br>Rule, |R| &lt; 10</li>
<li>No CRF, No Parsing tree, No Subjective filter </li>
</ul>
<h1 id="To-Do"><a href="#To-Do" class="headerlink" title="To Do"></a>To Do</h1><p>增加兩個不在 top feature 中的 attribute，但是在 pos/neg word weight 中的 n-grams 所評分的結果。在量化這些 n-grams 的分數時，不管正反面的強度，一律取絕對值進行加總，有可能一個正面單詞跟一個負面單詞合併在一起來表示一個更強烈的正面或反面資訊。</p>
<p>Training Classifier with 5000 subjective and 5000 objective processed sentences.</p>
<p>實作判斷主觀、客觀的分類器。</p>
<p><a href="http://www.cs.cornell.edu/People/pabo/movie-review-data/">http://www.cs.cornell.edu/People/pabo/movie-review-data/</a></p>
<h1 id="Github"><a href="#Github" class="headerlink" title="Github"></a>Github</h1><script type="text/javascript" src="http://morris821028.github.io/hexo-tag-oj/jquery.githubRepoWidget.js"></script><div class="github-widget" data-repo="morris821028/NLP-SentimentClassification"></div>

<p>感謝。</p>

        
      
    </div>

  </div>
  
    
<nav id="article-nav">
  
    <a href="/2015/02/28/diary-20150228/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          倒數第二次寒假
        
      </div>
    </a>
  
  
    <a href="/2015/01/14/lesson/civilization-project/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">[通識] 文明的進程 小組報告 2</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>
</section>
          
          
        
      </div>
      <footer id="footer">
  
  <div class="outer">    
    <div class="social-group">
      
      <a href="https://github.com/morris821028" target="_blank" title="github"><i class="icon-github"></i></a>
      
      
      <a href="https://www.facebook.com/Morris1028" target="_blank" title="facebook"><i class="icon-facebook-sign"></i></a>
      
      
      <a href="http://uhunt.felix-halim.net/id/46705" target="_blank" title="uhunt" ><span class="icon-uhunt">UVa<span></a>
      
    </div>
    <div id="footer-info" class="inner">
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and Theme by <a href="https://github.com/morris821028/hexo-theme-landscape" target="_blank" title="landscape">landscape</a> &copy; 2021 Shiang-Yun Yang 
    </div>
  </div>
</footer>


    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link"><i class=icon-home ></i>&nbsp&nbspHome</a>
  
    <a href="/about" class="mobile-nav-link"><i class=icon-user ></i>&nbsp&nbspAbout</a>
  
    <a href="/archives" class="mobile-nav-link"><i class=icon-archive ></i>&nbsp&nbspArchives</a>
  
    <a href="/tags" class="mobile-nav-link"><i class=icon-tags ></i>&nbsp&nbspTags</a>
  
    <a href="/picture" class="mobile-nav-link"><i class=icon-camera ></i>&nbsp&nbspPictures</a>
  
    <a href="/works" class="mobile-nav-link"><i class=icon-trophy ></i>&nbsp&nbspWorks</a>
  
</nav>
    
<script>
  var disqus_shortname = 'morris1028';
  
  var disqus_url = 'http://morris821028.github.io/2015/01/16/lesson/NLP-project/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//go.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/jquery.lazyload.js"></script>
<script src="/js/jquery.als-1.6.js"></script>

<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>